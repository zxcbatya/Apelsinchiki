# Архитектура нейронной сети и процесс обучения

## Обзор

Этот документ предоставляет подробное объяснение архитектуры нейронной сети и процесса обучения, используемого для прогнозирования вероятностей принятия заказов на поездки.

## Определение задачи

Мы стремимся прогнозировать P(is_done=1|x,price_bid_local), где:
- `is_done` - бинарная переменная (0: отмена, 1: выполнено)
- `x` представляет собой различные признаки заказа на поездку
- `price_bid_local` - предложенная водителем цена

## Предобработка данных

### Создание признаков

Мы создали несколько производных признаков для повышения производительности модели:

1. **Признаки, основанные на ценах**:
   - `price_increase_percent` = (price_bid_local - price_start_local) / price_start_local
   - `price_abs_diff` = price_bid_local - price_start_local
   - `price_per_km` = price_bid_local / (distance_in_meters / 1000)
   - `price_per_minute` = price_bid_local / (duration_in_seconds / 60)

2. **Признаки расстояния и продолжительности**:
   - `distance_km` = distance_in_meters / 1000
   - `duration_minutes` = duration_in_seconds / 60
   - `pickup_distance_km` = pickup_in_meters / 1000
   - `pickup_duration_minutes` = pickup_in_seconds / 60

### Очистка данных

- Пропущенные значения в `driver_rating` были заполнены медианным значением
- Категориальные переменные (`carmodel`, `carname`, `platform`) были закодированы с использованием label encoding
- Числовые признаки были стандартизированы с использованием StandardScaler

## Архитектура нейронной сети

### Входной слой
- **Размерность входных данных**: 15 признаков (числовые + закодированные категориальные)
- Признаки включают:
  - Необработанные признаки: distance_in_meters, duration_in_seconds, driver_rating и т.д.
  - Производные признаки: price_increase_percent, price_abs_diff, price_per_km и т.д.

### Скрытые слои
1. **Первый скрытый слой**:
   - **Нейроны**: 128
   - **Активация**: ReLU
   - **Регуляризация**: Dropout (0.3)

2. **Второй скрытый слой**:
   - **Нейроны**: 64
   - **Активация**: ReLU
   - **Регуляризация**: Dropout (0.3)

3. **Третий скрытый слой**:
   - **Нейроны**: 32
   - **Активация**: ReLU
   - **Регуляризация**: Dropout (0.2)

### Выходной слой
- **Нейроны**: 1
- **Активация**: Sigmoid (для бинарной классификации)
- **Выход**: Вероятность принятия заказа (от 0 до 1)

### Общее количество параметров
- **Обучаемые параметры**: ~12,000
- **Необучаемые параметры**: 0

## Компиляция модели

### Функция потерь
- **Бинарная кросс-энтропия**: Подходит для задач бинарной классификации
- Формула: L = -[y*log(p) + (1-y)*log(1-p)]
  - Где y - истинная метка, а p - прогнозируемая вероятность

### Оптимизатор
- **Adam**: Оптимизатор с адаптивной скоростью обучения
- **Скорость обучения**: По умолчанию (0.001)
- **Бета-значения**: β₁=0.9, β₂=0.999
- **Эпсилон**: 1e-7

### Метрики
- **Точность**: Основная метрика для оценки
- **Потери**: Мониторинг во время обучения

## Процесс обучения

### Разделение данных
- **Обучающая выборка**: 80% данных
- **Тестовая выборка**: 20% данных
- **Валидационное разделение**: 20% обучающих данных используется для валидации

### Гиперпараметры
- **Эпохи**: 50
- **Размер батча**: 32
- **Валидационное разделение**: 0.2

### Методы регуляризации
1. **Dropout**:
   - Применяется после каждого скрытого слоя
   - Ставки: 0.3, 0.3, 0.2 соответственно
   - Помогает предотвратить переобучение

2. **Ранняя остановка**:
   - Мониторинг валидационных потерь
   - Терпение: Не явно реализовано, но может быть добавлено

### Динамика обучения
- **Прямое распространение**: Вычисление прогнозов из входных признаков
- **Расчет потерь**: Сравнение прогнозов с фактическими метками
- **Обратное распространение**: Вычисление градиентов
- **Обновление параметров**: Корректировка весов с использованием оптимизатора Adam

## Метрики оценки

### Основные метрики
1. **Точность**: Процент правильно предсказанных меток
2. **ROC AUC**: Площадь под ROC-кривой для прогнозов вероятностей
3. **Точность и полнота**: Для детальной производительности классификации

### Вспомогательные метрики
1. **Потери**: Бинарная кросс-энтропия на тестовой выборке
2. **Матрица ошибок**: Истинные положительные, ложные положительные и т.д.

## Алгоритм оптимизации цен

После обучения модель используется для оптимизации цен:

### Шаги алгоритма
1. Для данного заказа с фиксированными признаками (кроме цены):
   - Определить диапазон кандидатов в цены
2. Для каждого кандидата в цены:
   - Обновить производные признаки, связанные с ценой
   - Прогнозировать вероятность принятия с использованием обученной модели
   - Рассчитать ожидаемый доход = цена × вероятность
3. Выбрать цену, максимизирующую ожидаемый доход

### Математическая формулировка
```
optimal_price = argmax_price(price × P(is_done=1|x,price))
```

Где:
- price - цена предложения
- P(is_done=1|x,price) - прогнозируемая моделью вероятность

## Валидация модели

### Стратегия кросс-валидации
- Однократное разделение train/test (80/20)
- Валидация во время обучения (20% обучающих данных)
- Может быть улучшена с помощью k-кратной кросс-валидации

### Индикаторы производительности
- Мониторинг потерь обучения и валидации
- Проверка на переобучение (расхождение кривых обучения/валидации)
- Оценка на невидимых тестовых данных

## Рекомендации по развертыванию

### Сохранение модели
- Сохранено как файл HDF5 (`pricing_model.h5`)
- Содержит веса, архитектуру и информацию о компиляции

### Конвейер вывода
1. Предобработка данных нового заказа (так же, как при обучении)
2. Применение масштабирования и кодирования
3. Прямой проход через сеть
4. Применение сигмоидальной активации для получения вероятности
5. Преобразование в бизнес-решение (принять/отклонить)

### Масштабируемость
- Размер модели: ~100KB
- Время вывода: Миллисекунды на прогноз
- Пакетная обработка: Возможна для нескольких заказов

## Возможные улучшения

### Улучшения архитектуры
1. **Глубже сеть**: Добавить больше скрытых слоев
2. **Шире слои**: Увеличить количество нейронов на слой
3. **Нормализация батча**: Нормализовать входы каждого слоя
4. **Разные активации**: Попробовать LeakyReLU, ELU и т.д.

### Улучшения обучения
1. **Планирование скорости обучения**: Уменьшить LR во время обучения
2. **Ранняя остановка**: Остановить, когда валидационные потери перестают улучшаться
3. **Регуляризация**: Добавить L1/L2 регуляризацию
4. **Аугментация данных**: Генерировать синтетические обучающие примеры

### Создание признаков
1. **Временные признаки**: Час суток, день недели
2. **Географические признаки**: Если бы были доступны данные о местоположении
3. **История водителя**: Предыдущие уровни принятия и т.д.
4. **История пользователя**: Предыдущие паттерны бронирования

## Заключение

Эта архитектура нейронной сети предоставляет прочный фундамент для прогнозирования вероятностей принятия заказов на поездки. Модель балансирует сложность и производительность, оставаясь при этом интерпретируемой. Компонент оптимизации цен напрямую решает бизнес-задачу максимизации дохода при поддержании высоких уровней принятия.